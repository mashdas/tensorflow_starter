{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pl_11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvqce6q/P0Voltnraj2Gmr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgrJJo8l2Bnb"
      },
      "source": [
        "##Vanishing Gradient-during backprop the error gradient is taken into account wrt each parameter and each learnable parameter is updated \n",
        "##accordingly..but at times..during backprop..the gradient keeps decreasing...and might even  fall to zero..due to which gradient descent update in the lower\n",
        "##layers leave them virtually unchanged\n",
        "\n",
        "##Exploding Gradient-Ulta of vanishing gradient....gradient keeps on increasing..weight updates become erratic and the model diverges "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9AUsmP7_gB1"
      },
      "source": [
        "##Solution-We want the signal to flow freely in both the directions...in fwd dir while making predictions and in reverse direction while backprop\n",
        "##FOR THE SIGNAL TO FLOW PROPERLY:\n",
        "##1.variance of op @each layer ==varaiance of ip@each layer\n",
        "##2.variance of gradients @ each layer should be same while flowing through a layer @ reverse\n",
        "##BUT THE ABOVE IS NOT POSSIBLE UNLESS THE LAYER HAS AN EQUAL NO. OF INPUTS(fan-in) AND NEURONS(fan-out)\n",
        "##COMPROMISE-weights are initialized randomly  based on Glorot inititalization\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWL8gDc3BQ7C"
      },
      "source": [
        "##GLOROT INITIALIZATION/XAVIER INITIALIZATION=\n",
        "##Normal distribution with mean 0 and variance sigma_2=1/fan_avg\n",
        "##uniform distribution between -r and +r,with r=sqrt(3/fan_avg)\n",
        "\n",
        "##Above..if we replace fan_avg with fan_in,we get LECUN Initialization(USED WITH SELU)--->scaled exponential linear unit\n",
        "\n",
        "\n",
        "##INITIALIZATION      ACTIVATION FUNCTIONS               variance\n",
        "##GLOROT              none,tanh,logistic,sftmax          1/fan_avg\n",
        "##He                  Relu(+the variants)                 2/fan_in\n",
        "##LeCun               SELU                                 1/fan_in\n",
        "\n",
        "##BY DEFAUL KERAS USES GLOROT INITIALIZATION WITH UNIFORM DISTRIBUTION\n",
        "\n",
        "## we can change this to He initialization via kernel_initializer\n",
        "\n",
        "#tf.keras.layers.Dense(10,activation=\"relu\",kernel_initializer=\"he_normal\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iHWluAbMnDN"
      },
      "source": [
        "#Problem with Relu\n",
        "\n",
        "\n",
        "# they might be suffering from Dying Relu.It occurs due to a large lr when the weights get tweaked in such a way thta \n",
        "# weighted sum of the inputs are negative for all the instances in the training set.(cuz for relu gradient of negative is 0)\n",
        "\n",
        "##SOLUTION:Leaky Relu\n",
        "Lrelu=max(alpha*z,z)\n",
        "alpha=how much the function leaks : tis the slope when z<0\n",
        "\n",
        "##Randomized Leaky Relu--->alpha is picked randomly during training and fixed to an average during testing\n",
        "\n",
        "##Parametric Leaky Relu--->alpha is learnable\n",
        "\n",
        "##ELU(z)={alpha(e**z-1) if z<0}\n",
        "##       {z if            z>=0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlOl5VtgPTVs"
      },
      "source": [
        "#heirarchy of activation functions\n",
        "\n",
        "#SELU>ELU>Leaky Relu>Relu>tanh>logistic"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NLfPoqPeeU"
      },
      "source": [
        "##TO use leaky Relu\n",
        "leaky_relu=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "layer=tf.keras.layers.Dense(10,activation=leaky_relu,kernel_initializer=\"he_normal\")\n",
        "\n",
        "##FOR SELU\n",
        "layer=tf.keras.layers.Dense(10,activation=\"selu\",kernerl_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K18FMTiESCF_"
      },
      "source": [
        "#BATCH NORMALIZATION\n",
        "\n",
        "introduced as a solution to vanisihing/exploding gradients\n",
        "\n",
        "what it does and where is it applied\n",
        "\n",
        "-before or after the activation function\n",
        "\n",
        "-a.zero centering and normalizing each input\n",
        "\n",
        "-b.scaling and shifting the result using two new parameter vectors\n",
        "\n",
        "NOTE:a and b is done per batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TosiC3CJSCJd"
      },
      "source": [
        "##NORMALIZATION IS BASICALLY---->centering the batch and normalizing them,then scaling and shifting them\n",
        "##the centering is done by substracting the batch_mean from the ip\n",
        "##the normalizing is done by dividing the above term by the sqrt(variance+epsilon)....epsilon is a small number..present there to prevent division by zero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUmcWfkGSCMv"
      },
      "source": [
        "##The output_scale vector  and the output_offset vector are learnable\n",
        "##NOTE--------------------------------------------------------------\n",
        "##the final ip_mean vector and ip_sd_vector are estimated during training,but they are not used at all during training,only after training"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COkd4d_hKJ_-"
      },
      "source": [
        "##One of the important hyperparameter of BatchNormalization is momentum(v)\n",
        "##This is used when updating the exponential moving averages\n",
        "##given a new 'v',(a new vector of input means or standard deviations computed over the current batch),running average V is updated via\n",
        "##V<--V*momentum+v(1-momentum)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoNHPrQbLBMo"
      },
      "source": [
        "##another important parameter is axis:determines which axis should be normalized\n",
        "##defaults to -1,i.e the last axis\n",
        "##Eg..2d ip batch-->[batch_size,features]---->axis=-1--->the features will be normalized\n",
        "\n",
        "##but if batchnormalizatoin is done before flatten,ip batch-->3d-->[batch_size,height,weight]\n",
        "##"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3fS8v0dSY5x"
      },
      "source": [
        "##GRADIENT CLIPPING\n",
        "\n",
        "optimizer=tf.keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\",optimizer=optimizer)\n",
        "\n",
        "this will clip every component of the gradient vector to a value -1.0 and 1.0 if it is not within the threshold\n",
        "\n",
        "clipvalue will change the orientation of the graient vector...better use clipnorm(preserves orientation)-->clips the whole gradient if \n",
        "l2 norm is greater than the threshold i picked."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYhGyt4HcvEZ"
      },
      "source": [
        "##MOMENTUM OPTIMIZATION --->takes into account the value of the previous gradient...unlike plain gradient descent\n",
        "##Tis like dis-->m<-Bm-ndj(0)/d0............ndj(0)/d0-->local gradient;m=momentum vector\n",
        "#--------------->0<-0+m\n",
        "#Hyparameters involved--->momentum:set between 0(high friction) and 1(no friction)--->typical_value=0.9\n",
        "\n",
        "##USAGE:\n",
        "##optimizer=tf.keras.optimizers.SGD(lr=0.001,momentum=0.9)\n",
        "\n",
        "Nesterov Acclerated Gradient==>better than vanilla moment====>only diff===> \n",
        "\n",
        "m<-Bm-ndj(0+Bm)/d0\n",
        "\n",
        "it measures the gradient slightly ahead in the direction of the momentum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7VVDZitrW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HnMZumdcvGs"
      },
      "source": [
        "\n",
        "##USING RMS PROP==>\n",
        "optimizer=tf.keras.optimizers.RMSprop(lr=0.001,rho=0.9)\n",
        "\n",
        "##USING ADAM\n",
        "optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIoLltFhtsDl"
      },
      "source": [
        "##LEARINNG RATE SCHEDULER"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM68Sc6JtsGJ"
      },
      "source": [
        "##power_scheduling--->r/2,r/3,r/4--->after every  s steps\n",
        "optimizer=keras.optimizers.SGD(lr=0.01,decay=1e-4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHflD82StsJj"
      },
      "source": [
        "##exponential scheduling\n",
        "\n",
        "def exponential_decay(lr0,s):\n",
        "  def exponential_decay_fn(epoch):\n",
        "    return 0.01*0.1**(epoch/20)\n",
        "  return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn=exponential_decay(lr=0.01,s=20)\n",
        "\n",
        "lr_scheduler=keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "\n",
        "##pass the scheduler to the callback option in fit option\n",
        "\n",
        "\n",
        "##pieceWise Scheduler\n",
        "\n",
        "def pw_c(epoch):\n",
        "  if epoch<5:\n",
        "    return 0.01\n",
        "  elif epoch<15:\n",
        "    return 0.005\n",
        "  else:\n",
        "    return 0.001\n",
        "\n",
        "pw_scheduler=keras.callbacks.LearningRateScheduler(pw_c)\n",
        "##SAME AS exponential scheduling\n",
        "\n",
        "##NOTE-------->IF OUR SCHEDULER USES EPOCH as input,we need to set the initial_epoch to proper value,if we are loading from a saved model that had lr_scheduler ip as epoch\n",
        "##otherwise the weights will get wrecked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Performance Scheduling\n",
        "\n",
        "lr_scheduler=keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=5)\n",
        "\n",
        "##if the best validation loss doesnt improve over 5 consecutive epochs,it will multiply the lr by 0.5\n",
        "\n",
        "\n",
        "\n",
        "##UPDATE THE LEARNING RATE AT EVERY S step instead of epoch\n",
        "\n",
        "s=20*len(X_train)\n",
        "lr=keras.optimizers.schedules.ExponentialDecay(0.01,s,0.1)\n",
        "\n",
        "opimizer=keras.optimizers.SGD(lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IzOeYoAtsM2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}